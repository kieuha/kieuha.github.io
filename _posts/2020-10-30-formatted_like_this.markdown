---
layout: post
title:      "formatted like this"
date:       2020-10-30 06:10:17 +0000
permalink:  formatted_like_this
---


Natural Language Processing 


The content of your blog post goes hereNATURAL LANGUAGE PROCESSING

Natural language processing (NLP) has a history dated back to the 1950s as the intersection of linguistics and artificial intelligence. In the beginning, NLP was information retrieval (IR) where relevant information was collected for a search. The information was mostly texts, but it could be sounds or images. One example of natural language processing is a translation from English to other languages and vice versa. However, the task is not as easy as it seems because of phonological, morphological, syntactic, and semantic complexity associate with languages in addition to how languages change over time. Most of the NLP work done during this period was about syntactic processing. To solve the problem with grammar, the Backus-Naur Form (BNF) or not a normal form was created in 1963. The purpose was to use symbols in place of texts, ![](http://)such as “:” is the same as “is equivalent to.” Many symbols and conventions were created. The converter is capable of producing types and parsers for abstract syntax in several languages, including Haskell and Java. The computing resources at the time were very limited and the computers were extremely slow. It could take 7 minutes to analyze long sentences. Despite the limitations, NLP research, particularly at Harvard, made good progress in the development of computational grammar with its lexicon and parsing strategy. 

The next phase of NLP work focused on artificial intelligence or AI for short. Ealy's work on AI was to solve problems with knowledge-base such as Baseball. For many years, managers picked the players and strategies for the games based on their intuition. In the middle of the 20th century, the sabermetric search began with the publishing of Earnshaw Cook. In 1977, Bill James, an American baseball writer, historian, and statistician who applied the statistics to pick players and strategies for the games. This also initiated a new phase of fresh ideas about other problems that AI might be able to solve. In the 1980s, IBM developed several successful, complicated statistical models. Many chatbot-style applications were developed, which could converse about restricted topics between humans and machines. Eventually, NLP systems research moved from rules-based approaches to statistical models. With the availability of the internet, text information becomes much more accessible. 

Today, NLP includes many statistical and machine learning methods for interpreting human language. The basic NLP task includes tokenization, parsing, and lemmatization or stemming sentiment analysis, and language detection. In other words, NLP is to break down language and build patterns that can create meaning. The applications of NLP include:
•	Content categorization: classify contents under a pre-classified category.
•	Topic discovery and modeling: extract the meaning and themes in texts
•	Context extraction: pull structured information from text-based sources
•	Sentiment analysis: analyze texts for mood and subjective opinions 
•	Document summarization: translate synopses of texts.
•	Machine translation: automatic translation from one language to another. Facebook is now can accurately translating between any pair of 100 languages without first translates into English. 
Step by Step to building an NLP model:
Data gathering: Data gathering can take a long time.
Sentence Segmentation: the first step after gathering the data is to break the text into separate sentences. 
Word tokenization: the next step is to break the sentence into separate words. Both sentences and word tokenizing work the same way. For sentence tokenization, each sentence is separated while for word tokenization, each word is separated. The idea is to analyze how frequent words occur and how different words associate with each other in a piece of writing, a book, etc. 
Stemming: To stem means to chop off the ends of words to remove derivational affixes.
Lemmatization: To lemmatize means to remove inflectional endings or to return words to their base form. Both methods are language-specific. 
Parsing: To organize text according to the grammar rule. 
Graphing: the common plotting seen in NLP is a frequency distribution. This provides visualization on what words occur frequently in the text of interest. To get a true representative of the word distribution, it is important to remove stopwords such as ‘the’, ‘as’, ‘a’, ‘an’, ‘are’, etc. These words occur frequently in English but they don’t give much clue what a piece of writing is about. These are a few common and also important parts of text processing..
